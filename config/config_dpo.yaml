policy_name: meta-llama/Llama-3.2-1B
ref_name: meta-llama/Llama-3.2-1B
precision: bf16
mount_dir: "/mnt/output"

dataset:
  dataset_name: Anthropic/hh-rlhf
  subset: train[:50%]
  val_ratio: 0.1
  seed: 42
  max_len: 512
  max_prompt_length: 128
  
dpo_training:
  epochs: 1
  batch_size: 32
  beta: 0.1
  learning_rate: 1e-6
  log_steps: 10

dpo_debug:
  enabled: true
  max_samples: 3
  output_dir: /logs/debugging

sft_training:
  enabled: true
  epochs: 1
  batch_size: 64
  eval_batch_size: 64
  learning_rate: 1e-5
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  log_steps: 5
  eval_steps: 100
  evaluation_strategy: steps
  dataloader_num_workers: 0
  debug_show_samples: true
  debug_samples: 5
  save_dir: ./logs/sft
  mount_save_dir: /mnt/output/sft_model
